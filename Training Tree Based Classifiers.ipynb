{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42460e1-60de-4ef0-97a5-9c6398e395f6",
   "metadata": {},
   "source": [
    "### Load Snowpark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5bd4557-d428-494b-8b92-9c74e417e730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowflake snowpark version is: (0, 10, 0)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import avg, sum, col,lit\n",
    "from snowflake.snowpark.functions import udf, sproc, col\n",
    "from snowflake.snowpark.types import IntegerType, FloatType, LongType, DoubleType, DecimalType,StringType, BooleanType, Variant\n",
    "from snowflake.snowpark.types import PandasSeries, PandasDataFrame\n",
    "from snowflake.snowpark import functions as fn\n",
    "\n",
    "import sys ,json\n",
    "import io\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from snowflake.snowpark import version\n",
    "print (f\"snowflake snowpark version is: {version.VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a80ae1-2be8-45ff-bf2e-e46918e944ee",
   "metadata": {},
   "source": [
    "### Connect to Snowflake and establish session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cf3189c-5b46-4b30-bd01-58cdc75717da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Database: \"BANK1_CRM_DB\"\n",
      "Current Schema: \"PUBLIC\"\n",
      "Current Warehouse: \"APP_WH\"\n"
     ]
    }
   ],
   "source": [
    "snowflake_connection_cfg = open('cred.json')\n",
    "snowflake_connection_cfg = snowflake_connection_cfg.read()\n",
    "snowflake_connection_cfg = json.loads(snowflake_connection_cfg)\n",
    "\n",
    "# Creating Snowpark Session\n",
    "tc_session = Session.builder.configs(snowflake_connection_cfg).create()\n",
    "print('Current Database:', tc_session.get_current_database())\n",
    "print('Current Schema:', tc_session.get_current_schema())\n",
    "print('Current Warehouse:', tc_session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9861-440b-4eba-973d-31ddd32ab497",
   "metadata": {},
   "source": [
    "### Create stage location for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07ca3d6a-67d0-44f9-b3d9-9ba62683e7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area STAGE_MODELS successfully created.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_session.sql(\"CREATE OR REPLACE STAGE stage_models\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30a72055-cf43-4b2a-8ca8-edce6b4fd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_session.clear_packages()\n",
    "tc_session.add_packages(\"snowflake-snowpark-python\")\n",
    "tc_session.add_packages(\"scikit-learn\",\"pandas\",\"numpy\",\"joblib\",\"cachetools\")\n",
    "tc_session.clear_imports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbeb19-2c36-4fea-8f29-962b6cb96fe4",
   "metadata": {},
   "source": [
    "### Define function to train test random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8401839a-2975-4b59-8c6a-fbcf062a186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(training_table: str, sample_size_n: int, features:list, Y:str, test_size:float, random_state:int):\n",
    "    # Loading data into pandas dataframe\n",
    "\n",
    "    # Define features and label\n",
    "    X = training_table[features]\n",
    "    Y = training_table[Y]\n",
    "\n",
    "    # Splitting data into training and test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a408a-8714-4888-aed6-dc0239fc91fb",
   "metadata": {},
   "source": [
    "### Define function to save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf3e1b08-20fa-4ec9-829f-feb1c47abe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(session, model, path, dest_filename):\n",
    "    # logger.debug('#save_file: -- START--')\n",
    "    input_stream = io.BytesIO()\n",
    "    joblib.dump(model, input_stream)\n",
    "    session._conn.upload_stream(input_stream, path, dest_filename)\n",
    "    return \"successfully created file: \" + path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfead7a3-0159-405a-9af1-084c44b551a2",
   "metadata": {},
   "source": [
    "### Define Features required to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c3a64bd-4c22-4ce6-9e51-0d0a513c652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['SEPTAL_LENGTH','SEPTAL_WIDTH','PETAL_LENGTH','PETAL_WIDTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba43c98-3092-4cb7-87cb-488d06c0276e",
   "metadata": {},
   "source": [
    "### Define Model pipeline for Imputer, Standard Scaler and Random Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b1a7471-4baa-4264-907b-cf1bd366efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rf_model(p_df: pd.DataFrame,ne,nj,cw, md):\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    numeric_features = p_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = p_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    feature_names = numeric_features + categorical_features\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler(with_mean=True,with_std=True))])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier'\n",
    "                    ,RandomForestClassifier(n_estimators=ne, n_jobs=-nj, class_weight=cw,max_depth=md)\n",
    "                    # ,RandomForestClassifier(n_estimators=4, n_jobs=-1, class_weight='balanced_subsample',max_depth=20)\n",
    "                    # ,RandomForestClassifier(maxBins=20,featureSubsetStrategy='onethird') need to find the equivalents\n",
    "                    # of these maxBins and featureSubsetStrategy. For featureSubsetStrategy I do think it is the \n",
    "                    # classweight from sklearn based on the documentation. I also think maxBins could be the same as\n",
    "                    # maxdepth.\n",
    "                )\n",
    "            ])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a945f889-f5b3-4f20-a3c0-1a222da635f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dtree_model(p_df: pd.DataFrame,cw, md):\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    numeric_features = p_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = p_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    feature_names = numeric_features + categorical_features\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler(with_mean=True,with_std=True))])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier'\n",
    "                    ,DecisionTreeClassifier(class_weight=cw,max_depth=md)\n",
    "                    # ,RandomForestClassifier(n_estimators=4, n_jobs=-1, class_weight='balanced_subsample',max_depth=20)\n",
    "                    # ,RandomForestClassifier(maxBins=20,featureSubsetStrategy='onethird') need to find the equivalents\n",
    "                    # of these maxBins and featureSubsetStrategy. For featureSubsetStrategy I do think it is the \n",
    "                    # classweight from sklearn based on the documentation. I also think maxBins could be the same as\n",
    "                    # maxdepth.\n",
    "                )\n",
    "            ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "849cb0ab-9421-47ac-bd0f-b0fc2037ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test, y_pred):\n",
    "    from sklearn import metrics\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict=True,target_names=['setosa', 'versicolor', 'virginica'])\n",
    "    df_classification_report = pd.DataFrame(report).transpose()    \n",
    "    return df_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4802883-d0ef-42ed-a7db-5c33baec86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model_name, test_size, random_state,ne,nj,cw,max_depth):\n",
    "    data = [[model_name,test_size,random_state,ne,nj,cw,max_depth]]  \n",
    "    df_model_info = pd.DataFrame(data,columns=['model','test_size','random_state','n_estimator','n_jobs','class_weight','max_depth'])\n",
    "    return df_model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fab21-0f9b-426e-a844-b22c51285c9e",
   "metadata": {},
   "source": [
    "### Train random forest classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9300bc8-2de4-409c-b45e-c47f3a192681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_model(session: Session, training_table: str, sample_size_n: int, model_name: str,features:list, Y: str,test_size:float,random_state:int,ne:int,nj:int,cw:str, md:int) -> str:\n",
    "    from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "    training_data = session.table(training_table).sample(n=sample_size_n).toPandas()\n",
    "\n",
    "    Data_train, Data_test, y_train, y_test = train_test_split(training_data, sample_size_n, features, Y,test_size,random_state)\n",
    "    from sklearn.ensemble import RandomForestClassifier \n",
    "    # Model building\n",
    "    rf = build_rf_model(Data_train,ne,nj,cw, md)\n",
    "    rf.fit(Data_train, y_train)\n",
    "\n",
    "    model_dir = '@stage_models'\n",
    "    model_fl = model_name+'.joblib'\n",
    "    save_file(session, rf, model_dir ,model_fl)\n",
    "\n",
    "    score = rf.score(Data_test, y_test)\n",
    "    \n",
    "    y_pred = rf.predict(Data_test)\n",
    "    df_classification_report = get_classification_report(y_pred,y_test).reset_index().rename(columns={\"index\": \"class\"}).reset_index(drop=True)\n",
    "    df_model_info = get_model_info(model_fl,test_size,random_state,ne,nj,cw,md)\n",
    "    df_model_info=df_model_info.append([df_model_info]*5,ignore_index=True)\n",
    "    # tc_session.create_dataframe(df_classification_report.join(df_model_info)).write.mode(\"overwrite\").save_as_table(\"model_output\")\n",
    "    \n",
    "    return df_classification_report.join(df_model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fab2d5b-63fd-45a1-a568-21d3b9d4c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dtree_model(session: Session, training_table: str, sample_size_n: int, model_name: str,features:list, Y: str,test_size:float,random_state:int,cw:str, md:int) -> str:\n",
    "    training_data = session.table(training_table).sample(n=sample_size_n).toPandas()\n",
    "\n",
    "    Data_train, Data_test, y_train, y_test = train_test_split(training_data, sample_size_n, features, Y,test_size,random_state)\n",
    "    # Model building\n",
    "    dtree = build_dtree_model(Data_train,cw, md)\n",
    "    dtree.fit(Data_train, y_train)\n",
    "\n",
    "    model_dir = '@stage_models'\n",
    "    model_fl = model_name+'.joblib'\n",
    "    save_file(session, dtree, model_dir ,model_fl)\n",
    "    \n",
    "    y_pred = dtree.predict(Data_test)\n",
    "    df_classification_report = get_classification_report(y_pred,y_test).reset_index().rename(columns={\"index\": \"class\"}).reset_index(drop=True)\n",
    "    df_model_info = get_model_info(model_fl,test_size,random_state,None,None,cw,md)\n",
    "    df_model_info=df_model_info.append([df_model_info]*5,ignore_index=True)\n",
    "    # tc_session.create_dataframe(df_classification_report.join(df_model_info)).write.mode(\"overwrite\").save_as_table(\"model_output\")\n",
    "    \n",
    "    return df_classification_report.join(df_model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4f52f-80c1-4584-96fb-22ecfedd4ed4",
   "metadata": {},
   "source": [
    "### Define stored proc to register random forest classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "344bd301-3b29-4590-a828-ae4c38cf5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the function as a Stored Procedure\n",
    "rf_sproc = tc_session.sproc.register(func=train_rf_model, # training function defined above\n",
    "                                            name='train_rf_model', # training model name to be registered in snowlake\n",
    "                                            is_permanent=True, # permanent stored proc\n",
    "                                            replace=True, # replace if existing already\n",
    "                                            stage_location='@stage_models', # save the model in stage location\n",
    "                                            packages=['snowflake-snowpark-python','scikit-learn','joblib']) # import model libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87dc43c2-a330-4fa5-bca6-5b72bc34ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the function as a Stored Procedure\n",
    "dtree_sproc = tc_session.sproc.register(func=train_dtree_model, # training function defined above\n",
    "                                            name='train_dtree_model', # training model name to be registered in snowlake\n",
    "                                            is_permanent=True, # permanent stored proc\n",
    "                                            replace=True, # replace if existing already\n",
    "                                            stage_location='@stage_models', # save the model in stage location\n",
    "                                            packages=['snowflake-snowpark-python','scikit-learn','joblib']) # import model libaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a236052-8026-44ed-a451-5916c7feb8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest classifier report\n",
      "          class  precision    recall  ...  n_jobs        class_weight max_depth\n",
      "0        setosa   1.000000  1.000000  ...       1  balanced_subsample        15\n",
      "1    versicolor   1.000000  0.666667  ...       1  balanced_subsample        15\n",
      "2     virginica   0.000000  0.000000  ...       1  balanced_subsample        15\n",
      "3      accuracy   0.800000  0.800000  ...       1  balanced_subsample        15\n",
      "4     macro avg   0.666667  0.555556  ...       1  balanced_subsample        15\n",
      "5  weighted avg   1.000000  0.800000  ...       1  balanced_subsample        15\n",
      "\n",
      "[6 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print (\"random forest classifier report\")\n",
    "print (rf_sproc('iris_dataset',100, 'rf_iris_model_ts10_md15',features,'LABEL',0.1,43,4,1,'balanced_subsample', 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cd48da1-a25a-4c0e-b27d-0bd67deaa525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest classifier report\n",
      "          class  precision    recall  ...  n_jobs        class_weight max_depth\n",
      "0        setosa   1.000000  1.000000  ...       1  balanced_subsample        20\n",
      "1    versicolor   1.000000  0.888889  ...       1  balanced_subsample        20\n",
      "2     virginica   0.900000  1.000000  ...       1  balanced_subsample        20\n",
      "3      accuracy   0.960000  0.960000  ...       1  balanced_subsample        20\n",
      "4     macro avg   0.966667  0.962963  ...       1  balanced_subsample        20\n",
      "5  weighted avg   0.964000  0.960000  ...       1  balanced_subsample        20\n",
      "\n",
      "[6 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print (\"random forest classifier report\")\n",
    "print (rf_sproc('iris_dataset',100, 'rf_iris_model_ts25_md20',features,'LABEL',0.25,43,4,1,'balanced_subsample', 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f85586b-3823-4c12-8e76-fb7bd0cf8534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest classifier report\n",
      "          class  precision    recall  ...  n_jobs        class_weight max_depth\n",
      "0        setosa   1.000000  1.000000  ...       1  balanced_subsample        25\n",
      "1    versicolor   1.000000  0.875000  ...       1  balanced_subsample        25\n",
      "2     virginica   0.888889  1.000000  ...       1  balanced_subsample        25\n",
      "3      accuracy   0.960000  0.960000  ...       1  balanced_subsample        25\n",
      "4     macro avg   0.962963  0.958333  ...       1  balanced_subsample        25\n",
      "5  weighted avg   0.964444  0.960000  ...       1  balanced_subsample        25\n",
      "\n",
      "[6 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print (\"random forest classifier report\")\n",
    "print (rf_sproc('iris_dataset',100, 'rf_iris_model_ts25_md25',features,'LABEL',0.25,43,4,1,'balanced_subsample', 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fc4aa79-88be-4254-bb05-165b1944eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree classifier report\n",
      "          class  precision    recall  ...  n_jobs  class_weight max_depth\n",
      "0        setosa   1.000000  1.000000  ...    None      balanced        20\n",
      "1    versicolor   0.875000  1.000000  ...    None      balanced        20\n",
      "2     virginica   1.000000  0.888889  ...    None      balanced        20\n",
      "3      accuracy   0.960000  0.960000  ...    None      balanced        20\n",
      "4     macro avg   0.958333  0.962963  ...    None      balanced        20\n",
      "5  weighted avg   0.965000  0.960000  ...    None      balanced        20\n",
      "\n",
      "[6 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print (\"decision tree classifier report\")\n",
    "print (dtree_sproc('iris_dataset',100, 'dtree_iris_model_ts25_md20',features,'LABEL',0.25,43,'balanced', 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f024852-594f-413d-887d-4e5a2433dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree classifier report\n",
      "          class  precision    recall  ...  n_jobs  class_weight max_depth\n",
      "0        setosa   1.000000  1.000000  ...    None      balanced        25\n",
      "1    versicolor   0.833333  0.625000  ...    None      balanced        25\n",
      "2     virginica   0.625000  0.833333  ...    None      balanced        25\n",
      "3      accuracy   0.840000  0.840000  ...    None      balanced        25\n",
      "4     macro avg   0.819444  0.819444  ...    None      balanced        25\n",
      "5  weighted avg   0.856667  0.840000  ...    None      balanced        25\n",
      "\n",
      "[6 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print (\"decision tree classifier report\")\n",
    "print (dtree_sproc('iris_dataset',100, 'dtree_iris_model_ts25_md25',features,'LABEL',0.25,43,'balanced', 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71ca9165-98b9-4442-ae54-4a829c456442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree classifier report\n",
      "          class  precision    recall  ...  n_jobs  class_weight max_depth\n",
      "0        setosa   1.000000  1.000000  ...    None      balanced        30\n",
      "1    versicolor   0.888889  1.000000  ...    None      balanced        30\n",
      "2     virginica   1.000000  0.800000  ...    None      balanced        30\n",
      "3      accuracy   0.960000  0.960000  ...    None      balanced        30\n",
      "4     macro avg   0.962963  0.933333  ...    None      balanced        30\n",
      "5  weighted avg   0.964444  0.960000  ...    None      balanced        30\n",
      "\n",
      "[6 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print (\"decision tree classifier report\")\n",
    "print (dtree_sproc('iris_dataset',100, 'dtree_iris_model_ts25_md30',features,'LABEL',0.25,43,'balanced', 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7a9c1-05ab-4bd6-bfa7-e324f53bc932",
   "metadata": {},
   "source": [
    "### You can also Train multiple random forest classifier using registered stored proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf07f596-dec7-4acc-bfc4-55c25be0aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size = np.arange(0.30, 0.34, 0.01)\n",
    "# random_state = np.arange(22, 25, 1)\n",
    "# max_depth = np.arange(10,35,5)\n",
    "# for t in test_size:\n",
    "#     for r in random_state:\n",
    "#         for m in max_depth:\n",
    "#             print (f\"random forest classifier for test size : {t}, random state : {r} and max_depth :{m}\")\n",
    "#             print (rf_sproc('iris_dataset',150, 'rf_iris_model',features,'LABEL',float(t),int(r),4,1,'balanced_subsample', int(m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6f039-5a42-414f-bcb8-5962a0915d54",
   "metadata": {},
   "source": [
    "### Check if the classifier models are saved in stage location.\n",
    "### Remember if the same model name was used for all the iterations, then only the last trained model will be saved\n",
    "### for the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2067e1fd-60e6-462b-a0bb-9f44688b1e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='stage_models/dtree_iris_model_ts25_md20.joblib', size=4464, md5='55699d68aef93e8d437bae7c9bca2f6a', last_modified='Mon, 28 Nov 2022 03:30:55 GMT'),\n",
       " Row(name='stage_models/dtree_iris_model_ts25_md25.joblib', size=4464, md5='f260ad381d643f7133073755b228a9ea', last_modified='Mon, 28 Nov 2022 03:31:09 GMT'),\n",
       " Row(name='stage_models/dtree_iris_model_ts25_md30.joblib', size=4624, md5='0da182b87ef07e4bf7cf7879700ea556', last_modified='Mon, 28 Nov 2022 03:31:25 GMT'),\n",
       " Row(name='stage_models/rf_iris_model.joblib', size=7536, md5='dd9c3bb79ef0a030e89b6bce8b3bd716', last_modified='Mon, 28 Nov 2022 03:27:55 GMT'),\n",
       " Row(name='stage_models/rf_iris_model_ts10_md15.joblib', size=9456, md5='1d266b594d99225c43631f5836e96879', last_modified='Mon, 28 Nov 2022 03:29:34 GMT'),\n",
       " Row(name='stage_models/rf_iris_model_ts25_md20.joblib', size=10576, md5='18cb306df02eb278e265e6762eab5c0e', last_modified='Mon, 28 Nov 2022 03:29:41 GMT'),\n",
       " Row(name='stage_models/rf_iris_model_ts25_md25.joblib', size=8816, md5='e60bba0bc796fc7a0f3e8fbb789202ad', last_modified='Mon, 28 Nov 2022 03:30:09 GMT'),\n",
       " Row(name='stage_models/train_dtree_model/udf_py_1770627365.zip', size=2816, md5='1e86268a64ccd1f4eb864ba25f3aab83', last_modified='Mon, 28 Nov 2022 03:27:45 GMT'),\n",
       " Row(name='stage_models/train_rf_model/udf_py_1732425029.zip', size=2944, md5='f3250a8f07cc96c55e925b4d62d3f830', last_modified='Mon, 28 Nov 2022 03:27:38 GMT')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_session.sql(\"list @stage_models\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ad2b8e3-7e14-4a49-8ec3-35132f43ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!!!\n"
     ]
    }
   ],
   "source": [
    "tc_session.close()\n",
    "print('Finished!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256179af-2c6c-4cc4-a633-395019236439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
